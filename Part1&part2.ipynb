{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "k = 10  \n",
    "n_problems = 1000  \n",
    "n_steps = 1000  \n",
    "epsilons = [0.1, 0.01, 0.0] \n",
    "alpha = 0.1  \n",
    "optimistic_initial_value = 5.0  \n",
    "\n",
    "# Function to generate bandit problems\n",
    "def generate_bandit_problems(k, n_problems):\n",
    "    true_means = np.random.normal(0, 1, (n_problems, k))\n",
    "    return true_means\n",
    "\n",
    "# Function to run a single bandit problem\n",
    "def run_bandit(true_means, n_steps, method, epsilon=0.1, alpha=0.1, optimistic_initial_value=5.0):\n",
    "    k = len(true_means)\n",
    "    q_estimates = np.zeros(k)\n",
    "    action_counts = np.zeros(k)\n",
    "    total_reward = 0\n",
    "    optimal_action_counts = 0\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_action_taken = np.zeros(n_steps)\n",
    "    optimal_action = np.argmax(true_means)\n",
    "    \n",
    "    if method == 'optimistic':\n",
    "        q_estimates.fill(optimistic_initial_value)\n",
    "    if method == 'gradient':\n",
    "        preferences = np.zeros(k)\n",
    "        action_probabilities = np.ones(k) / k\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        if method == 'greedy':\n",
    "            action = np.argmax(q_estimates)\n",
    "        elif method == 'epsilon-greedy':\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'optimistic':\n",
    "            action = np.argmax(q_estimates)\n",
    "        elif method == 'gradient':\n",
    "            action = np.random.choice(k, p=action_probabilities)\n",
    "        \n",
    "        reward = np.random.normal(true_means[action], 1)\n",
    "        total_reward += reward\n",
    "        action_counts[action] += 1\n",
    "        rewards[step] = reward\n",
    "        if action == optimal_action:\n",
    "            optimal_action_counts += 1\n",
    "        optimal_action_taken[step] = optimal_action_counts / (step + 1)\n",
    "        \n",
    "        if method == 'gradient':\n",
    "            one_hot = np.zeros(k)\n",
    "            one_hot[action] = 1\n",
    "            baseline = total_reward / (step + 1)\n",
    "            preferences += alpha * (reward - baseline) * (one_hot - action_probabilities)\n",
    "            action_probabilities = np.exp(preferences) / np.sum(np.exp(preferences))\n",
    "        else:\n",
    "            q_estimates[action] += (reward - q_estimates[action]) / action_counts[action]\n",
    "    \n",
    "    return rewards, optimal_action_taken\n",
    "\n",
    "# Main simulation\n",
    "true_means = generate_bandit_problems(k, n_problems)\n",
    "methods = ['greedy', 'optimistic', 'gradient']\n",
    "results = {method: {'rewards': np.zeros((n_problems, n_steps)), 'optimal_action': np.zeros((n_problems, n_steps))} for method in methods}\n",
    "\n",
    "# Initialize results for epsilon-greedy with different epsilons\n",
    "for epsilon in epsilons:\n",
    "    results[f'epsilon-greedy-{epsilon}'] = {'rewards': np.zeros((n_problems, n_steps)), 'optimal_action': np.zeros((n_problems, n_steps))}\n",
    "\n",
    "for problem in range(n_problems):\n",
    "    for method in methods:\n",
    "        rewards, optimal_action_taken = run_bandit(true_means[problem], n_steps, method, alpha=alpha, optimistic_initial_value=optimistic_initial_value)\n",
    "        results[method]['rewards'][problem] = rewards\n",
    "        results[method]['optimal_action'][problem] = optimal_action_taken\n",
    "    for epsilon in epsilons:\n",
    "        rewards, optimal_action_taken = run_bandit(true_means[problem], n_steps, 'epsilon-greedy', epsilon=epsilon)\n",
    "        results[f'epsilon-greedy-{epsilon}']['rewards'][problem] = rewards\n",
    "        results[f'epsilon-greedy-{epsilon}']['optimal_action'][problem] = optimal_action_taken\n",
    "\n",
    "# Averaging results\n",
    "average_rewards = {method: np.mean(results[method]['rewards'], axis=0) for method in results}\n",
    "optimal_action_percentages = {method: np.mean(results[method]['optimal_action'], axis=0) for method in results}\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 8))\n",
    "for method in average_rewards:\n",
    "    plt.plot(average_rewards[method], label=f'Average Reward - {method}')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.title('Average Reward per Step')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for method in optimal_action_percentages:\n",
    "    plt.plot(optimal_action_percentages[method], label=f'Optimal Action % - {method}')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Optimal Action %')\n",
    "plt.legend()\n",
    "plt.title('Optimal Action Percentage per Step')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "k = 10\n",
    "n_problems = 1000\n",
    "n_steps = 20000\n",
    "epsilon_fixed = 0.1\n",
    "epsilon_decreasing_initial = 0.1\n",
    "alpha = 0.1  # For fixed step-size epsilon-greedy\n",
    "optimistic_initial_value = 5.0\n",
    "\n",
    "# Function to initialize bandit problems\n",
    "def initialize_bandit(k):\n",
    "    means = np.random.normal(0, 1, k)\n",
    "    return means\n",
    "\n",
    "# Function to apply drift change\n",
    "def apply_drift_change(means):\n",
    "    means += np.random.normal(0, 0.001, k)\n",
    "    return means\n",
    "\n",
    "# Function to apply mean-reverting change\n",
    "def apply_mean_reverting_change(means):\n",
    "    means = 0.5 * means + np.random.normal(0, 0.01, k)\n",
    "    return means\n",
    "\n",
    "# Function to apply abrupt change\n",
    "def apply_abrupt_change(means):\n",
    "    if np.random.rand() < 0.005:\n",
    "        np.random.shuffle(means)\n",
    "    return means\n",
    "\n",
    "# Function to run a bandit problem with a specific method\n",
    "def run_bandit(method, k, n_steps, initial_means, epsilon=None, alpha=None, optimistic_value=None):\n",
    "    q_estimates = np.zeros(k)\n",
    "    if method == 'optimistic':\n",
    "        q_estimates.fill(optimistic_value)\n",
    "    action_counts = np.zeros(k)\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_action_counts = np.zeros(n_steps)\n",
    "\n",
    "    means = initial_means.copy()\n",
    "    optimal_action = np.argmax(means)\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        if method == 'epsilon_fixed':\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'epsilon_decreasing':\n",
    "            epsilon_t = epsilon_decreasing_initial / (t + 1)\n",
    "            if np.random.rand() < epsilon_t:\n",
    "                action = np.random.randint(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'optimistic':\n",
    "            action = np.argmax(q_estimates)\n",
    "\n",
    "        reward = np.random.normal(means[action], 1)\n",
    "        action_counts[action] += 1\n",
    "        rewards[t] = reward\n",
    "        optimal_action_counts[t] = (action == optimal_action)\n",
    "\n",
    "        if method == 'epsilon_fixed':\n",
    "            q_estimates[action] += alpha * (reward - q_estimates[action])\n",
    "        else:\n",
    "            q_estimates[action] += (reward - q_estimates[action]) / action_counts[action]\n",
    "\n",
    "        means = apply_drift_change(means)  # Apply drift change\n",
    "        means = apply_abrupt_change(means)  # Apply abrupt change\n",
    "        optimal_action = np.argmax(means)\n",
    "\n",
    "    return rewards, optimal_action_counts\n",
    "\n",
    "# Running experiments\n",
    "methods = ['optimistic', 'epsilon_fixed', 'epsilon_decreasing']\n",
    "results = {method: [] for method in methods}\n",
    "\n",
    "for method in methods:\n",
    "    for _ in range(n_problems):\n",
    "        initial_means = initialize_bandit(k)\n",
    "        if method == 'optimistic':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, optimistic_value=optimistic_initial_value)\n",
    "        elif method == 'epsilon_fixed':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, epsilon=epsilon_fixed, alpha=alpha)\n",
    "        elif method == 'epsilon_decreasing':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, epsilon=epsilon_decreasing_initial)\n",
    "        results[method].append(rewards)\n",
    "\n",
    "# Plotting the results\n",
    "for method in methods:\n",
    "    final_rewards = [result[-1] for result in results[method]]\n",
    "    plt.boxplot(final_rewards, positions=[methods.index(method)], widths=0.6)\n",
    "\n",
    "plt.xticks(range(len(methods)), methods)\n",
    "plt.ylabel('Final Reward Distribution')\n",
    "plt.title('Comparison of Methods on Non-Stationary Bandit Problem')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
