{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: The task involves solving a k-armed bandit problem with 10 arms, each having normally distributed rewards with unknown means. The goal is to learn the expected rewards for each arm using four different methods: (1) a greedy algorithm with non-optimistic initial values, (2) an epsilon-greedy algorithm with varying epsilon values, (3) a greedy algorithm with optimistic initial values, and (4) a gradient bandit algorithm with different learning rates. For each method, the performance is evaluated over 1000 bandit problems by tracking the average reward and the percentage of optimal actions taken at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "k = 10  \n",
    "n_problems = 1000  \n",
    "n_steps = 1000  \n",
    "epsilons = [0.1, 0.01, 0.0] \n",
    "alpha = 0.1  \n",
    "optimistic_initial_value = 5.0  \n",
    "\n",
    "# Function to generate bandit problems\n",
    "def generate_bandit_problems(k, n_problems):\n",
    "    true_means = np.random.normal(0, 1, (n_problems, k))\n",
    "    return true_means\n",
    "\n",
    "'''\n",
    "    This function is the main function to run the bandit problem. It defines a k-armed bandit problem with true_means as the true mean values of the k arms.\n",
    "    We initially use the environment value that's set up above.\n",
    "    The main idea is to pull multiple arms and get rewards from them. We then update the q_estimates for each arm based on the rewards we get. \n",
    "    The method used here is optimistic, gradient, greedy and epsilon-greedy. \n",
    "'''\n",
    "def run_bandit(true_means, n_steps, method, epsilon=0.1, alpha=0.1, optimistic_initial_value=5.0):\n",
    "    k = len(true_means)\n",
    "    q_estimates = np.zeros(k)\n",
    "    action_counts = np.zeros(k)\n",
    "    total_reward = 0\n",
    "    optimal_action_counts = 0\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_action_taken = np.zeros(n_steps)\n",
    "    optimal_action = np.argmax(true_means)\n",
    "    \n",
    "    if method == 'optimistic':\n",
    "        q_estimates.fill(optimistic_initial_value) # Assume every arm has a high value initially\n",
    "    if method == 'gradient':\n",
    "        preferences = np.zeros(k)\n",
    "        action_probabilities = np.ones(k) / k # Set preference based on the reward then update\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        if method == 'greedy': # Always get the biggest reward\n",
    "            action = np.argmax(q_estimates)\n",
    "        elif method == 'epsilon-greedy': # Be greedy or explore with predefined epsilon\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'optimistic':\n",
    "            action = np.argmax(q_estimates)\n",
    "        elif method == 'gradient':\n",
    "            action = np.random.choice(k, p=action_probabilities)\n",
    "        \n",
    "        reward = np.random.normal(true_means[action], 1)\n",
    "        total_reward += reward\n",
    "        action_counts[action] += 1\n",
    "        rewards[step] = reward\n",
    "        if action == optimal_action:\n",
    "            optimal_action_counts += 1\n",
    "        optimal_action_taken[step] = optimal_action_counts / (step + 1)\n",
    "        \n",
    "        if method == 'gradient':\n",
    "            one_hot = np.zeros(k)\n",
    "            one_hot[action] = 1\n",
    "            baseline = total_reward / (step + 1)\n",
    "            preferences += alpha * (reward - baseline) * (one_hot - action_probabilities)\n",
    "            action_probabilities = np.exp(preferences) / np.sum(np.exp(preferences))\n",
    "        else:\n",
    "            q_estimates[action] += (reward - q_estimates[action]) / action_counts[action]\n",
    "    \n",
    "    return rewards, optimal_action_taken\n",
    "\n",
    "# Main simulation\n",
    "true_means = generate_bandit_problems(k, n_problems)\n",
    "methods = ['greedy', 'optimistic', 'gradient']\n",
    "results = {method: {'rewards': np.zeros((n_problems, n_steps)), 'optimal_action': np.zeros((n_problems, n_steps))} for method in methods}\n",
    "\n",
    "# Initialize results for epsilon-greedy with different epsilons\n",
    "for epsilon in epsilons:\n",
    "    results[f'epsilon-greedy-{epsilon}'] = {'rewards': np.zeros((n_problems, n_steps)), 'optimal_action': np.zeros((n_problems, n_steps))}\n",
    "\n",
    "# Loop through all problems and methods\n",
    "for problem in range(n_problems):\n",
    "    for method in methods:\n",
    "        rewards, optimal_action_taken = run_bandit(true_means[problem], n_steps, method, alpha=alpha, optimistic_initial_value=5)\n",
    "        results[method]['rewards'][problem] = rewards\n",
    "        results[method]['optimal_action'][problem] = optimal_action_taken\n",
    "    for epsilon in epsilons:\n",
    "        rewards, optimal_action_taken = run_bandit(true_means[problem], n_steps, 'epsilon-greedy', epsilon=epsilon)\n",
    "        results[f'epsilon-greedy-{epsilon}']['rewards'][problem] = rewards\n",
    "        results[f'epsilon-greedy-{epsilon}']['optimal_action'][problem] = optimal_action_taken\n",
    "\n",
    "# Averaging results\n",
    "average_rewards = {method: np.mean(results[method]['rewards'], axis=0) for method in results}\n",
    "optimal_action_percentages = {method: np.mean(results[method]['optimal_action'], axis=0) for method in results}\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 8))\n",
    "for method in average_rewards:\n",
    "    plt.plot(average_rewards[method], label=f'Average Reward - {method}')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.title('Average Reward per Step')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for method in optimal_action_percentages:\n",
    "    plt.plot(optimal_action_percentages[method], label=f'Optimal Action % - {method}')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Optimal Action %')\n",
    "plt.legend()\n",
    "plt.title('Optimal Action Percentage per Step')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: The task is to address non-stationary modifications of the k-armed bandit problem, where reward distributions change over time either gradually or abruptly. For gradual changes, two scenarios are considered: (1) a drift change where the mean parameters follow \"u*t = u*{t-1} + epsilon*t\" with \"epsilon_t ~ N(0, 0.001^2)\", and (2) a mean-reverting change where \"u_t = k \\* u*{t-1} + epsilon_t\" with \"k = 0.5\" and \"epsilon_t ~ N(0, 0.01^2)\". For abrupt changes, at each time step, with a probability of 0.005, the means of the reward distributions are permuted. The evaluation involves comparing three methods: (1) an optimistic greedy approach, (2) epsilon-greedy with a fixed step size, and (3) epsilon-greedy with a decreasing step size. These methods are tested over 1000 repetitions of the non-stationary problems, and the distribution of average rewards at a terminal step (e.g., after 10,000 or 20,000 steps) is analyzed using box plots. The algorithm that yields the most favorable reward distribution at the terminal step is considered preferable. Pilot runs are recommended to fine-tune parameters before the main experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "k = 10\n",
    "n_problems = 1000\n",
    "n_steps = 20000\n",
    "epsilon_fixed = 0.1\n",
    "epsilon_decreasing_initial = 0.1\n",
    "alpha = 0.1 \n",
    "optimistic_initial_value = 5.0\n",
    "\n",
    "\n",
    "def initialize_bandit(k):\n",
    "    means = np.random.normal(0, 1, k)\n",
    "    return means\n",
    "\n",
    "#Functions for non-stationary bandit problems\n",
    "# Function to apply drift change\n",
    "def apply_drift_change(means):\n",
    "    means += np.random.normal(0, 0.001, k)\n",
    "    return means\n",
    "\n",
    "# Function to apply mean-reverting change\n",
    "def apply_mean_reverting_change(means):\n",
    "    means = 0.5 * means + np.random.normal(0, 0.01, k)\n",
    "    return means\n",
    "\n",
    "# Function to apply abrupt change\n",
    "def apply_abrupt_change(means):\n",
    "    if np.random.rand() < 0.005:\n",
    "        np.random.shuffle(means)\n",
    "    return means\n",
    "\n",
    "# Main function to run the bandit problem\n",
    "def run_bandit(method, k, n_steps, initial_means, epsilon=None, alpha=None, optimistic_value=None):\n",
    "    q_estimates = np.zeros(k)\n",
    "    if method == 'optimistic':\n",
    "        q_estimates.fill(optimistic_value)\n",
    "    action_counts = np.zeros(k)\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_action_counts = np.zeros(n_steps)\n",
    "\n",
    "    means = initial_means.copy()\n",
    "    optimal_action = np.argmax(means)\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        if method == 'epsilon_fixed':\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'epsilon_decreasing':\n",
    "            epsilon_t = epsilon_decreasing_initial / (t + 1)\n",
    "            if np.random.rand() < epsilon_t:\n",
    "                action = np.random.randint(k)\n",
    "            else:\n",
    "                action = np.argmax(q_estimates)\n",
    "        elif method == 'optimistic':\n",
    "            action = np.argmax(q_estimates)\n",
    "\n",
    "        reward = np.random.normal(means[action], 1)\n",
    "        action_counts[action] += 1\n",
    "        rewards[t] = reward\n",
    "        optimal_action_counts[t] = (action == optimal_action)\n",
    "\n",
    "        if method == 'epsilon_fixed':\n",
    "            q_estimates[action] += alpha * (reward - q_estimates[action])\n",
    "        else:\n",
    "            q_estimates[action] += (reward - q_estimates[action]) / action_counts[action]\n",
    "\n",
    "        means = apply_drift_change(means)  \n",
    "        means = apply_abrupt_change(means)  \n",
    "        optimal_action = np.argmax(means)\n",
    "\n",
    "    return rewards, optimal_action_counts\n",
    "\n",
    "# Running experiments\n",
    "methods = ['optimistic', 'epsilon_fixed', 'epsilon_decreasing']\n",
    "results = {method: [] for method in methods}\n",
    "\n",
    "# Loop through all methods and problems\n",
    "for method in methods:\n",
    "    for _ in range(n_problems):\n",
    "        initial_means = initialize_bandit(k)\n",
    "        if method == 'optimistic':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, optimistic_value=optimistic_initial_value)\n",
    "        elif method == 'epsilon_fixed':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, epsilon=epsilon_fixed, alpha=alpha)\n",
    "        elif method == 'epsilon_decreasing':\n",
    "            rewards, optimal_action_counts = run_bandit(method, k, n_steps, initial_means, epsilon=epsilon_decreasing_initial)\n",
    "        results[method].append(rewards)\n",
    "\n",
    "# Plotting the results\n",
    "for method in methods:\n",
    "    final_rewards = [result[-1] for result in results[method]]\n",
    "    plt.boxplot(final_rewards, positions=[methods.index(method)], widths=0.6)\n",
    "\n",
    "plt.xticks(range(len(methods)), methods)\n",
    "plt.ylabel('Final Reward Distribution')\n",
    "plt.title('Comparison of Methods on Non-Stationary Bandit Problem')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
